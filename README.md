# Adaptaci√≥n de Textos para el Aprendizaje del Espa√±ol üìö

## Introducci√≥n ‚úçÔ∏è

Este proyecto tiene como objetivo desarrollar un modelo de lenguajes que tenga la capacidad de adaptar textos castellanos a los distintos niveles de complejidad establecidos por el MCER o "Marco Com√∫n Europeo de Referencia para las Lenguas". Los niveles del MCER son los siguientes: A1, A2, B1, B2, C1, C2 (de principiante hasta avanzado). Para la adaptaci√≥n de los textos, estos deben ajustarse apropiadamente a la complejidad ling√º√≠stica solicitada y conservar su significado original.

## Hip√≥tesis de Trabajo ü§î

Antes de iniciar el desarrollo del proyecto, planteamos las siguientes hip√≥tesis:
- Todo texto puede clasificarse dentro de un nivel de competencia ling√º√≠stica seg√∫n el MCER.
- Es posible reescribir un texto en distintos niveles manteniendo su significado esencial.

## Objetivos Preliminares üéØ

El desarrollo del proyecto se basa en los siguientes objetivos:
1. Obtener un conjunto de datos con textos en espa√±ol etiquetados por nivel de competencia ling√º√≠stica (A1, A2, B1, B2, C1, C2).
2. Desarrollar y entrenar un clasificador de texto que identifique el nivel MCER en espa√±ol, permitiendo la comparaci√≥n con modelos generadores de texto.
3. Evaluar modelos de lenguaje para la adaptaci√≥n de textos en espa√±ol:
    - Comparar la precisi√≥n y velocidad de adaptaci√≥n de cada modelo.
    - Seleccionar el modelo con mejor rendimiento.
4. Realizar un ajuste fino (*fine-tuning*) del modelo de lenguaje seleccionado para mejorar su desempe√±o en la tarea de adaptaci√≥n de textos.

## Desarrollo del Proyecto üîß

### 1. Dataset üìÇ  

El primer paso fue obtener un conjunto de datos con textos etiquetados seg√∫n los niveles del MCER. Como no encontramos un dataset adecuado en espa√±ol, optamos por traducir el dataset **[CEFR Levelled English Texts (Kaggle)](https://www.kaggle.com/datasets/amontgomerie/cefr-levelled-english-texts)** del ingl√©s al espa√±ol.  

| ![Original](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.001.png) | ![Traducido](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.002.png) |
|:--------------------------------:|:--------------------------------:|
| Texto original en ingl√©s         | Texto traducido al espa√±ol      |  

Para la traducci√≥n utilizamos el modelo [opus-mt-en-es](https://huggingface.co/Helsinki-NLP/opus-mt-en-es). A continuaci√≥n, se presentan los benchmarks de este modelo en distintos conjuntos de prueba, lo que nos da una idea del margen de error que puede introducir la traducci√≥n:  

| Conjunto de prueba               | BLEU  | chr-F |
|----------------------------------|-------|-------|
| newssyscomb2009-engspa.eng.spa  | 31.0  | 0.583 |
| news-test2008-engspa.eng.spa    | 29.7  | 0.564 |
| newstest2009-engspa.eng.spa     | 30.2  | 0.578 |
| newstest2010-engspa.eng.spa     | 36.9  | 0.620 |
| newstest2011-engspa.eng.spa     | 38.2  | 0.619 |
| newstest2012-engspa.eng.spa     | 39.0  | 0.625 |
| newstest2013-engspa.eng.spa     | 35.0  | 0.598 |
| Tatoeba-test.eng.spa            | 54.9  | 0.721 |

#### ¬øQu√© significan BLEU y chr-F?  
- **BLEU (Bilingual Evaluation Understudy)**: Es una m√©trica que eval√∫a la calidad de una traducci√≥n autom√°tica compar√°ndola con una traducci√≥n humana de referencia. Analiza cu√°ntas palabras y frases coinciden con la versi√≥n correcta. Un puntaje m√°s alto (entre 0 y 100) indica una traducci√≥n m√°s precisa.  
- **chr-F (Character F-score)**: En lugar de comparar palabras completas, esta m√©trica analiza fragmentos de palabras y caracteres. Es √∫til cuando la traducci√≥n no es exacta pero sigue transmitiendo el mismo significado, lo que ayuda a evaluar mejor la calidad del resultado.

Ahora con este dataset traducido realizaremos un **balanceo de clases** tomando como referencia la clase minoritaria (C2 = 200).

![Distribuci√≥n de clases balanceada](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.003.png)

Este balanceo ayuda a evitar que el modelo favorezca las clases mayoritarias e ignore patrones importantes en las minoritarias.


### 2. Clasificador üîç

Antes de evaluar los modelos de lenguaje, es necesario contar con un **clasificador de texto** que permita comparar la precisi√≥n de los resultados. Sin embargo, no pudimos encontrar un clasificador de textos el cual fuera espec√≠fico para niveles del MCER en castellano. Por este motivo es que decidimos entrenar uno propio, utilizando nuestro dataset previamente etiquetado.

#### Enfoques de Entrenamiento üî¨

Probamos tres enfoques distintos para entrenar el clasificador:

- **BERT** ü§ñ
- **Red Neuronal FeedForward** üß†
- **LinearSVC** ‚öôÔ∏è

##### **1. BERT** ü§ñ
BERT es un modelo basado en *transformers* que comprende el contexto de las palabras dentro de una oraci√≥n. Utilizamos DistilBERT, una versi√≥n ligera de BERT, para clasificar los textos seg√∫n su nivel MCER. El proceso incluy√≥:

- Tokenizaci√≥n de los textos con un *tokenizer* preentrenado.
- Conversi√≥n del dataset a un formato compatible con Hugging Face.
- Ajuste fino (*fine-tuning*) del modelo para la clasificaci√≥n de secuencias.
- Entrenamiento con *cross-entropy loss* y evaluaci√≥n con *accuracy*.

##### **2. Red Neuronal FeedForward** üß†
Este enfoque implementa una red neuronal densa para la clasificaci√≥n de textos. Sus principales componentes incluyen:

- **Capa de entrada**: recibe representaciones vectoriales de los textos.
- **Capas ocultas**: utilizan activaciones ReLU para capturar patrones no lineales.
- **Capa de salida**: una funci√≥n *softmax* que asigna probabilidades a cada nivel MCER.

El modelo se entren√≥ mediante *backpropagation*, optimizaci√≥n con Adam y *cross-entropy loss* categ√≥rica. Sin embargo, este enfoque requiere cierto conocimiento en dise√±o de redes neuronales, algo que nosotros no tenemos y los resultados (como consecuencia) fueron deficientes, con un 37% de precisi√≥n en el mejor de los casos.

##### **3. LinearSVC** ‚öôÔ∏è
Este enfoque emplea un clasificador de *M√°quinas de Soporte Vectorial* (SVM) con los siguientes pasos:

- Vectorizaci√≥n TF-IDF para convertir los textos en representaciones num√©ricas.
- Entrenamiento de un modelo LinearSVC para encontrar un hiperplano que separe los textos por nivel MCER.
- Evaluaci√≥n con m√©tricas de clasificaci√≥n, incluyendo *accuracy* y *recall*.

Si bien LinearSVC fue sencillo de implementar y ejecutar, sus resultados fueron similares a los obtenidos con la red neuronal FeedForward.

##### **Resultados y Elecci√≥n del Modelo**

Luego de realizar los respectivos entrenamientos, obtuvimos el mejor rendimiento con BERT. No presentaremos todos los resultados de los clasificadores, ya que el an√°lisis de clasificadores de texto no forma parte del objetivo de este proyecto, pero s√≠ mencionaremos los par√°metros de entrenamiento que dieron los mejores resultados con BERT, en caso de que se desee replicar este clasificador:

- 75% de los datos para entrenamiento y 25% para pruebas.
- 12 √©pocas ‚è≥.
- **Batch size** = 32.

Aplicados de la siguiente forma:

```python
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=12,
    weight_decay=0.1,
    eval_strategy="epoch",
    logging_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()
```

### Precisi√≥n del Clasificador con el Enfoque BERT üìä

![](images/metrics_histogram_eval.png)

Al graficar la precisi√≥n de cada clase, se observa que nuestro clasificador enfrenta mayores dificultades en los niveles intermedios, especialmente en la clase **C1**.

Esta problem√°tica se refleja con mayor claridad en la siguiente **matriz de confusi√≥n**:

![](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.007.png)

A pesar de esto, es importante notar que la mayor√≠a de estos errores se producen en niveles muy cercanos (principalmente aquellos niveles que son adyacentes), esto implicar√≠a que el desfase es en realidad leve (este tipo de imprecisiones es algo que pueden experimentar incluso los profesionales del √°rea). Por lo tanto, la **precisi√≥n del 66%** corresponde exclusivamente a aquellas predicciones que llamaremos **predicciones exactas**.

### Precisi√≥n Aproximada üìà

Para una mejor evaluaci√≥n del desempe√±o de nuestro clasificador, introduciremos una m√©trica de inter√©s llamada **Precisi√≥n Aproximada**. Esta m√©trica utiliza el siguiente criterio:

- **Correcci√≥n = | nivel_esperado ‚àí nivel_predecido |**
- Si la correcci√≥n es **0**, la predicci√≥n es **correcta** (1 punto). ‚úÖ
- Si la correcci√≥n es **1**, la predicci√≥n es **aproximadamente correcta** o **adyacente** (0.5 puntos). ü§î
- En cualquier otro caso, la predicci√≥n es **incorrecta** (0 puntos). ‚ùå

Es decir, si bien la **Precisi√≥n Exacta** sigue siendo del **66%**, podemos tomar en consideraci√≥n tambi√©n la  **Precisi√≥n Aproximada**, la cual alcanza un **96%** de precisi√≥n. üéØ

Si bien estos resultados son alentadores, es importante considerar que la traducci√≥n del conjunto de datos introduce cierto nivel de ruido en el entrenamiento, lo que puede afectar la precisi√≥n del clasificador.

#### Ejemplo de uso de Clasificador en espa√±ol

Luego de realizar el entrenamiento, guradamos el modelo de la siguiente forma:

```python
trainer.save_model('path of the classifier folder')
tokenizer.save_pretrained('path of the classifier folder')
```

Cargamos y usamos el modelo de una manera similar:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_path = "path of the classifier folder"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
```

Ahora necesitamos seleccionar un texto. Supongamos que queremos clasificar el siguiente texto obtenido del dataset de la anterior seccion (un texto de nivel A1):

```
Esto es divertido, pap√°.
S√≠, lo es. Vamos a pescar.
¬øQu√© debo hacer?
Sostenga su poste sobre el agua.
¬øAs√≠?
S√≠. ¬°Mira! Cogiste un pez.
¬°Pap√°! ¬øQu√© hago?
Espera, hijo. Conseguir√© una red.
```

Para ello, realizamos lo siguiente:

```python
import torch

# Dataset path: 0. Dataset/spanish_dataset200.csv
text = df["text"][198] 

# Tokenizaci√≥n del texto
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

# Inferencia del modelo
with torch.no_grad():
    outputs = model(**inputs)

# Obtenci√≥n de los logits
logits = outputs.logits

# Convertir logits en probabilidades usando softmax
probabilities = torch.nn.functional.softmax(logits, dim=-1)

# Obtener el √≠ndice de la clase con la mayor probabilidad
predicted_class = torch.argmax(probabilities, dim=-1).item()
levels = ["A1", "A2", "B1", "B2", "C1", "C2"]
predicted_level = levels[predicted_class]

print(predicted_level)
```

Y con esto obtenemos como salida el label predicho (En este caso: **A1**).


### Comparaci√≥n con Clasificador en Ingl√©s üåç

Para evaluar el impacto del **ruido de traducci√≥n**, entrenamos un clasificador en ingl√©s utilizando los mismos par√°metros y un conjunto de datos balanceado. Luego del entrenamiento, obtuvimos estos resultados:

![](images/metrics_histogram_test.png) 

Como se puede observar, el clasificador en ingl√©s enfrenta dificultades similares a las del clasificador en espa√±ol. Para comprender mejor estos resultados, es fundamental analizar su **matriz de confusi√≥n**:  

![](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.010.jpeg)  

La similitud entre las **matrices de confusi√≥n** de ambos clasificadores (ingl√©s y espa√±ol) sugiere que el impacto en la traducci√≥n no resulta ser muy diferencial en el entrenamiento del clasificador cuando observamos la precisi√≥n obtenida y la comparamos.

Recordemos que cualquier posible ruido en los datos proviene del modelo de traducci√≥n utilizado. Adem√°s, es importante tener en cuenta que el propio clasificador tambi√©n contribuye con su **error de entrenamiento**, lo que puede afectar a la precisi√≥n general del sistema en las siguientes secciones.  


### 3. Modelos de lenguaje para generaci√≥n de texto üåê

Probamos varios modelos, pero la mayor√≠a result√≥ ser demasiado pesada, lenta o simplemente no interpretaba bien el prompt. Afortunadamente, encontramos dos modelos que destacaron por su facilidad de uso, rapidez, buena documentaci√≥n y capacidad para comprender la tarea asignada:

- **Mistral** ü§ñ
- **Cohere** üåê

Ambos funcionan mediante API, lo que significa que el procesamiento (incluyendo las peticiones por medio de un prompt, el fine-tuning, entre otros) se realiza en los servidores de sus desarrolladores y no en nuestras computadoras.

Para ambos modelos, utilizamos el siguiente prompt:

```python
prompt = lambda label, text: f"""
A continuaci√≥n, te proporcionar√© un texto en espa√±ol y te pedir√© que lo modifiques para diferentes niveles de competencia ling√º√≠stica
(A1, A2, B1, B2, C1 y C2), concretamente: {label}. El objetivo es que adaptes el texto seg√∫n el nivel de dificultad, modificando el
vocabulario y las estructuras gramaticales para que se ajusten a cada nivel, pero manteniendo el mismo mensaje central. Solo responde
con la versi√≥n del texto modificada para dicho nivel. No incluyas ninguna introducci√≥n, t√≠tulo, explicaci√≥n o comentario. Solamente dame
el texto adaptado.

Aqu√≠ est√° el texto:
{text}
"""
```

En este caso, **label** representa el nivel ling√º√≠stico deseado para la modificaci√≥n del texto, y **text** es el texto base que se adaptar√°.

#### Ejemplo de uso de Mistral ü§ñ
A continuaci√≥n, mostramos un ejemplo de c√≥mo usamos Mistral con nuestro prompt

```python
from mistralai import Mistral

mistral_cli = Mistral(api_key="API_KEY_MISTRAL")

def run_mistral(user_message, model="mistral-large-latest"):
    messages = [
        {"role": "user", "content": user_message}
    ]
    chat_response = mistral_cli.chat.complete(
        model=model,
        messages=messages
    )
    return chat_response.choices[0].message.content
```

Siendo **user_message** el prompt con el texto y el nivel elegidos.

Ahora, dado el siguiente texto en nivel A1:

```
Jenna estaba en el aeropuerto. Estaba esperando su avi√≥n. Su avi√≥n partir√≠a a las 19:00 horas. Eran solo las dos de la tarde. Ella tuvo tiempo de comer. Tuvo tiempo para estudiar. Fue al restaurante del aeropuerto. El restaurante estaba en el tercer piso. El restaurante estaba lleno. No hab√≠a asientos vac√≠os. No hab√≠a mesas vac√≠as. Ella no quer√≠a estar en la fila. No quer√≠a esperar. Hab√≠a otro restaurante en el aeropuerto. Estaba en el primer piso. Baj√≥ al primer piso. Ese restaurante estaba casi vac√≠o. Hab√≠a muchos asientos y muchas mesas.
```

Le pediremos a Mistral que adapte este texto a A2 de la siguiente forma:

```
A continuaci√≥n, te proporcionar√© un texto en espa√±ol y te pedir√© que lo modifiques para diferentes niveles de competencia ling√º√≠stica (A1, A2, B1, B2, C1 y C2), concretamente: B2. El objetivo es que adaptes el texto seg√∫n el nivel de dificultad, modificando el vocabulario y las estructuras gramaticales para que se ajusten a cada nivel, pero manteniendo el mismo mensaje central. Solo responde con la version del texto modificada para dicho nivel. No incluyas ninguna introducci√≥n, t√≠tulo, explicaci√≥n o comentario. Solamente dame el texto adaptado.

Aqu√≠ est√° el texto:
Jenna estaba en el aeropuerto. Estaba esperando su avi√≥n. Su avi√≥n partir√≠a a las 19.00 horas. Eran s√≥lo las dos de la tarde. Ella tuvo tiempo de comer. Tuvo tiempo para estudiar. Fue al restaurante del aeropuerto. El restaurante estaba en el tercer piso. El restaurante estaba lleno. No hab√≠a asientos vac√≠os. No hab√≠a mesas vac√≠as. Ella no quer√≠a estar en la fila. No quer√≠a esperar. Hab√≠a otro restaurante en el aeropuerto. Estaba en el primer piso. Baj√≥ al primer piso. Ese restaurante estaba casi vac√≠o. Hab√≠a muchos asientos y muchas mesas.
```

El resultado obtenido es:

```
Jenna estaba en el aeropuerto esperando su avi√≥n, que saldr√≠a a las 19:00 horas. Eran solo las dos de la tarde, as√≠ que decidi√≥ comer algo y estudiar un poco. Se dirigi√≥ al restaurante del tercer piso, pero estaba lleno y no hab√≠a asientos disponibles. Como no quer√≠a hacer cola ni esperar, baj√≥ al primer piso, donde encontr√≥ otro restaurante casi vac√≠o con muchos asientos y mesas libres.
```

#### Ejemplo de uso de Cohere üåê
A continuaci√≥n, mostramos un ejemplo de c√≥mo usamos Cohere con nuestro prompt

```python
import cohere

def run_cohere(user_message, model="command"):
    co = cohere.Client("API_KEY_COHERE")
    response = co.chat(message=user_message)
    return response.text
```

Siendo **user_message** el prompt con el texto y el nivel elegidos.

Dado el siguiente texto en nivel A1:

```
Jenna estaba en el aeropuerto. Estaba esperando su avi√≥n. Su avi√≥n partir√≠a a las 19:00 horas. Eran solo las dos de la tarde. Ella tuvo tiempo de comer. Tuvo tiempo para estudiar. Fue al restaurante del aeropuerto. El restaurante estaba en el tercer piso. El restaurante estaba lleno. No hab√≠a asientos vac√≠os. No hab√≠a mesas vac√≠as. Ella no quer√≠a estar en la fila. No quer√≠a esperar. Hab√≠a otro restaurante en el aeropuerto. Estaba en el primer piso. Baj√≥ al primer piso. Ese restaurante estaba casi vac√≠o. Hab√≠a muchos asientos y muchas mesas.
```

Le pediremos a Cohere que adapte este texto a A2 (Igual que con Mistral), obteniendo el siguiente resultado:

```
Jenna se encontraba en el aeropuerto, aguardando la salida de su vuelo programado para las 19:00 horas. Al ser apenas las dos de la tarde, decidi√≥ aprovechar el tiempo. Despu√©s de comer, se dispuso a estudiar. Se dirigi√≥ al restaurante del aeropuerto, ubicado en el tercer piso, pero se encontr√≥ con que estaba abarrotado; no hab√≠a ni un asiento ni una mesa libre. No deseando esperar, record√≥ que hab√≠a otro restaurante en el primer piso. Al bajar, se alegr√≥ de ver que este restaurante estaba casi desierto, con abundantes asientos y mesas disponibles.
```

### Evaluaci√≥n de los modelos üìä

Para evaluar y comparar estos dos modelos, tomamos una muestra de 120 textos y asignamos aleatoriamente un nivel de competencia ling√º√≠stica a cada uno. Luego, procesamos estos textos con el prompt en cada modelo. La evaluaci√≥n se bas√≥ en tres m√©tricas principales: tiempo de ejecuci√≥n, precisi√≥n exacta y precisi√≥n aproximada. (Cabe destacar que estas dos √∫ltimas m√©tricas fueron obtenidas al evaluar el texto adaptado por los modelos en un clasificador previamente entrenado, lo que introduce un margen de error del 34% en la precisi√≥n exacta y del 4% en la aproximada).

#### Mistral ü§ñ

* **Tiempo de ejecuci√≥n:** 47 minutos.
* **Precisi√≥n exacta:** 20.83%.
* **Precisi√≥n aproximada:** 37.91%.

Los resultados muestran que las predicciones de Mistral son bastante deficientes, con un rendimiento por debajo de 0.5. Esto se puede visualizar en la siguiente matriz de confusi√≥n:

![](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.013.png)

#### Cohere üåê

* **Tiempo de ejecuci√≥n:** 16 minutos.
* **Precisi√≥n exacta:** 39.17%.
* **Precisi√≥n aproximada:** 55%.

Como se observa en la matriz de confusi√≥n, **Cohere** supera a **Mistral** en todos los aspectos, interpretando de manera m√°s efectiva el prompt y generando textos de mayor calidad.

![](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.015.png)

Se evidencia la dificultad para diferenciar los casos intermedios, especialmente en las clases B1 y B2, as√≠ como en C2. Considerando el tiempo de ejecuci√≥n y los resultados obtenidos, **Cohere** se presenta como la mejor opci√≥n para hacer el Fine-Tuning.

### üìå Fine-Tuning del Modelo Cohere para la Adaptaci√≥n de Textos  

#### üîç Proceso  

El primer paso es generar un dataset con ejemplos estructurados de la siguiente manera (aplicando el mismo prompt que usamos para comparar Mistra y Cohere en la secci√≥n anterior):  

```json
[
  { "role": "User", "content": "Prompt con un texto y el nivel pedido" },
  { "role": "Chatbot", "content": "Texto generado por Cohere" }
]
```  

Para esto, seguimos el siguiente flujo:  

1. **Generaci√≥n del dataset base para fine-tuning:**  
   - Seleccionamos aleatoriamente 1,200 textos del dataset original.  
   - Usamos Cohere para adaptarlos a distintos niveles del MCER, asegurando un balance de 200 textos por nivel.  

2. **Creaci√≥n de subdatasets con diferentes filtros:**  
   - **`Sin filtros`**: Incluye el dataset original sin modificaciones (resultado del paso anterior).  
   - **`Exactos`**: Solo los textos en los que el clasificador asigna el mismo nivel que el solicitado.  
   - **`Exactos y Adyacentes`**: Textos donde el clasificador asigna el nivel exacto o uno adyacente al solicitado.  
   - **`Exactos + Mitad de Textos Adyacentes`**: Similar al anterior, pero conserva solo la mitad de los textos adyacentes.  

Es importante destacar que este proceso arrastra dos fuentes de error:  
- **Ruido por la traducci√≥n:** Posibles alteraciones en los textos al traducir el dataset original.  
- **Error del clasificador:** Desajustes entre el nivel solicitado y el nivel clasificado en los textos adaptados.  

#### üìä Resultados  

A continuaci√≥n, se presentan los resultados obtenidos con los distintos datasets generados.  

##### üìå Dataset Completo (`Sin filtro`)  

```
üîπ Precisi√≥n Exacta: 31%  
üîπ Precisi√≥n Aproximada: 52.2%  
üîπ Cantidad de Textos: 1,200  
```

| Nivel | Cantidad |
|-------|---------|
| A1    | 208     |
| A2    | 293     |
| B1    | 193     |
| B2    | 208     |
| C1    | 199     |
| C2    | 99      |

Se observa una inconsistencia en la distribuci√≥n de textos por nivel, ya que, aunque se solicit√≥ adaptar exactamente 200 textos por nivel, el clasificador final asign√≥ cantidades diferentes. Esto indica que algunos textos no fueron adaptados correctamente al nivel esperado.  

##### ‚úÖ Dataset `Exactos` (Adaptaciones Coincidentes)  

```
üîπ Precisi√≥n Exacta: 100%  
üîπ Precisi√≥n Aproximada: 100%  
üîπ Cantidad de Textos: 372  
```

| Nivel | Cantidad |
|-------|---------|
| A1    | 116     |
| A2    | 85      |
| B1    | 41      |
| B2    | 50      |
| C1    | 45      |
| C2    | 35      |

Este resultado refleja un patr√≥n ya observado al comparar Mistral con Cohere: la facilidad del modelo para adaptar textos a niveles bajos y su p√©rdida de precisi√≥n a medida que aumenta la complejidad del nivel solicitado. Como consecuencia, el dataset final est√° desbalanceado, con una mayor proporci√≥n de textos en niveles b√°sicos.  

##### üéØ Dataset `Exactos y Adyacentes`  

```
üîπ Precisi√≥n Exacta: 42.2%  
üîπ Precisi√≥n Aproximada: 71.1%  
üîπ Cantidad de Textos: 881  
```

| Nivel | Cantidad |
|-------|---------|
| A1    | 166     |
| A2    | 214     |
| B1    | 116     |
| B2    | 144     |
| C1    | 172     |
| C2    | 69      |

Este dataset ampl√≠a el de **Exactos**, incorporando ejemplos en los que el clasificador asign√≥ un nivel adyacente al solicitado. Esto permite aumentar la cantidad de datos disponibles sin perder demasiada precisi√≥n (o al menos, no tanta precisi√≥n como el dataset sin filtros).  

##### üî• Dataset `Exactos + Mitad de Textos Adyacentes`  

```
üîπ Precisi√≥n Exacta: 59.3%  
üîπ Precisi√≥n Aproximada: 79.7%  
üîπ Cantidad de Textos: 627  
```

| Nivel | Cantidad |
|-------|---------|
| A1    | 142     |
| A2    | 149     |
| B1    | 80      |
| B2    | 94      |
| C1    | 109     |
| C2    | 53      |

Este dataset es una versi√≥n reducida del **Exactos y Adyacentes**, donde solo se conserva la mitad de los textos adyacentes. El objetivo es evaluar si reducir esta variabilidad mejora la precisi√≥n del modelo sin comprometer la diversidad de los datos.  

![](images/Aspose.Words.ccf872ce-c988-4e7e-8645-db3a81b14ce5.024.jpeg)  

La imagen muestra la base de datos original de Cohere junto con los datasets generados en este proceso.  

#### üöÄ Fine-Tuning en Cohere  

Despu√©s de cargar el archivo JSON del dataset en la plataforma de Cohere, obtenemos el **ID del dataset** y procedemos a realizar el fine-tuning con el siguiente c√≥digo:  

```python
import cohere

# Clave de API de Cohere
apikey = "API_KEY_COHERE"

# Inicializar cliente de Cohere
co = cohere.Client(apikey)  # Obt√©n tu API Key en: https://dashboard.cohere.com/api-keys

from cohere.finetuning import (
    BaseModel,
    FinetunedModel,
    Hyperparameters,
    Settings,
    WandbConfig
)

# Configuraci√≥n de hiperpar√°metros
hp = Hyperparameters(
    early_stopping_patience=10,  # Detiene el entrenamiento si la m√©trica de p√©rdida no mejora despu√©s de 10 evaluaciones.
    early_stopping_threshold=0.001,  # Umbral de mejora m√≠nima para evitar la detenci√≥n temprana.
    train_batch_size=16,  # Tama√±o del lote de entrenamiento (entre 2 y 16).
    train_epochs=10,  # N√∫mero m√°ximo de √©pocas de entrenamiento (entre 1 y 10).
    learning_rate=0.001,  # Tasa de aprendizaje (entre 0.00005 y 0.1).
)

# Configuraci√≥n de Weights & Biases (opcional)
wnb_config = WandbConfig(
    project="test-project",
    api_key=apikey,
    entity="test-entity",
)

# Creaci√≥n del modelo ajustado
finetuned_model = co.finetuning.create_finetuned_model(
    request=FinetunedModel(
        name="exacto",
        settings=Settings(
            base_model=BaseModel(base_type="BASE_TYPE_CHAT"),
            dataset_id="ID_DATASET_EXACTO",  # Reemplazar con el ID del dataset subido
            hyperparameters=hp,
            wandb=wnb_config,
        ),
    )
)
```  

Este c√≥digo configura y entrena un modelo ajustado en Cohere utilizando el dataset generado. Se establecen hiperpar√°metros clave como la tasa de aprendizaje, el tama√±o del lote y la estrategia de detenci√≥n temprana para optimizar el rendimiento del modelo.  

#### üî¨ Resultados del Fine-Tuning  

Tras entrenar los modelos con los distintos datasets generados, evaluamos su rendimiento mediante el siguiente procedimiento:  

1. **Selecci√≥n de textos:** Elegimos aleatoriamente 240 textos (40 por cada nivel del MCER).  
2. **Asignaci√≥n de nivel:** Seleccionamos aleatoriamente el nivel al que deseamos adaptar cada texto.  
3. **Generaci√≥n de respuestas:** Pasamos el prompt con el texto y el nivel objetivo al modelo fine-tuneado.  
4. **Evaluaci√≥n:** Clasificamos los textos generados y comparamos el nivel predicho con el nivel deseado.  

Es importante recordar que la evaluaci√≥n en este √∫ltimo paso est√° sujeta a los errores del clasificador, los cuales pueden provenir de:  

- **Ruido en el dataset**, debido al proceso de traducci√≥n.  
- **Errores en el entrenamiento**, que afectan la precisi√≥n de las predicciones.  


#### üìä Desempe√±o por Dataset  

**üéØ `Exactos`**  
```
üîπ Precisi√≥n Exacta: 18.3% (44 de 240 textos acertados) 
üîπ Precisi√≥n Aproximada: 27.1% (65 de 240 textos aproximados)  
```

**üéØ `Exactos y Adyacentes`**  
```
üîπ Precisi√≥n Exacta: 18.3% (44 de 240 textos acertados)  
üîπ Precisi√≥n Aproximada: 35.8% (86 de 240 textos aproximados)  
```

**üéØ `Exactos + Mitad de Adyacentes`**  
```
üîπ Precisi√≥n Exacta: 19.2% (46 de 240 textos acertados)  
üîπ Precisi√≥n Aproximada: 39.2% (94 de 240 textos aproximados)   
```

**üéØ `Sin Filtros`**  
```
üîπ Precisi√≥n Exacta: 17.5% (42 de 240 textos acertados)  
üîπ Precisi√≥n Aproximada: 36.7% (88 de 240 textos aproximados)   
```

Estos resultados reflejan el impacto de los distintos filtros aplicados en el dataset. Aunque la eliminaci√≥n de ruido mejora ligeramente la precisi√≥n, los valores obtenidos indican que el modelo a√∫n enfrenta desaf√≠os en la adaptaci√≥n precisa de los textos a los niveles deseados.  

## üöÄ Conclusiones

üìå **El enfoque m√°s prometedor** es entrenar con las adaptaciones exactas del clasificador (Dataset Exactos). A pesar de ser la muestra m√°s peque√±a, sus resultados fueron similares a conjuntos m√°s grandes (Exacto + Mitad de Adyacentes tuvo un leve mejor desempe√±o, pero con casi el triple de datos). Recordemos nuevamente el error introducido por la traducci√≥n y por el clasificador que terminan afectando al resultado final.

‚ö†Ô∏è **Limitaciones:**
- Cohere restringe las llamadas a la API (m√°ximo 1000 por mes), lo que dificulta generar y filtrar m√°s textos.
- La versi√≥n gratuita de Cohere impone restricciones en los par√°metros de Fine-Tuning.
- Carencia de un dataset adecuado para la tarea.
- Clasificador con pobre desempe√±o.

## üìå Posibles Mejoras

* Crear un dataset original en castellano con niveles etiquetados, evitando traducciones.
* Elegir un mejor modelo de lenguaje base que sea gratuito o considerar pagar por acceso sin restricciones de API ni Fine-Tuning.
* Entrenar un Clasificador con una mayor precisi√≥n. 

üöÄ Si tuvi√©ramos m√°s tiempo y recursos, podr√≠amos seguir introduciendo prompts, filtrando los mejores seg√∫n el clasificador y refinando el entrenamiento hasta obtener adaptaciones satisfactorias. El mayor obst√°culo fue la limitaci√≥n impuesta por Cohere en la versi√≥n gratuita.


## Referencias üìö

- **Modelos de Lenguaje**:
  - [Mistral AI | Frontier AI in your hands](https://mistral.ai/)
  - [Cohere | The leading AI platform for enterprise](https://cohere.com/)
  
- **Dataset en Ingl√©s**: [CEFR Levelled English Texts (Kaggle)](https://www.kaggle.com/datasets/amontgomerie/cefr-levelled-english-texts)

- **Traductor EN-ES**: [Helsinki-NLP/opus-mt-en-es ¬∑ Hugging Face](https://huggingface.co/Helsinki-NLP/opus-mt-en-es)

- **Traducci√≥n (C√≥digo)**: [notebook9834025409 | Kaggle](https://www.kaggle.com/code/alexistomascenteno/notebook9834025409/edit/run/197471934)
